{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7a79568",
   "metadata": {},
   "source": [
    "Our objective was to augment authors to add the appropriate tags for their project so the community can discover them. So we want to use the metadata provided in each project to determine what the relevant tags are. We'll want to start with the highly influential features and iteratively experiment with additional features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c105aa",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec00a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "import ipywidgets as widgets\n",
    "import itertools\n",
    "import json\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c0d322e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": 2106,\n",
      "  \"created_on\": \"2020-08-08 15:06:18\",\n",
      "  \"title\": \"Fast NST for Videos (+ person segmentation) \\ud83c\\udfa5 + \\u26a1\\ud83d\\udcbb + \\ud83c\\udfa8 = \\u2764\\ufe0f\",\n",
      "  \"description\": \"Create NST videos and pick separate styles for the person in the video and for the background.\",\n",
      "  \"tags\": [\n",
      "    \"code\",\n",
      "    \"tutorial\",\n",
      "    \"video\",\n",
      "    \"computer-vision\",\n",
      "    \"style-transfer\",\n",
      "    \"neural-style-transfer\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load projects\n",
    "url = \"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/datasets/projects.json\"\n",
    "projects = json.loads(urlopen(url).read())\n",
    "print (json.dumps(projects[-305], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a69598a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2032 projects\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_on</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-02-17 06:30:41</td>\n",
       "      <td>Machine Learning Basics</td>\n",
       "      <td>A practical set of notebooks on machine learni...</td>\n",
       "      <td>[code, tutorial, keras, pytorch, tensorflow, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-02-17 06:41:45</td>\n",
       "      <td>Deep Learning with Electronic Health Record (E...</td>\n",
       "      <td>A comprehensive look at recent machine learnin...</td>\n",
       "      <td>[article, tutorial, deep-learning, health, ehr]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2020-02-20 06:07:59</td>\n",
       "      <td>Automatic Parking Management using computer vi...</td>\n",
       "      <td>Detecting empty and parked spaces in car parki...</td>\n",
       "      <td>[code, tutorial, video, python, machine-learni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2020-02-20 06:21:57</td>\n",
       "      <td>Easy street parking using region proposal netw...</td>\n",
       "      <td>Get a text on your phone whenever a nearby par...</td>\n",
       "      <td>[code, tutorial, python, pytorch, machine-lear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2020-02-20 06:29:18</td>\n",
       "      <td>Deep Learning based parking management system ...</td>\n",
       "      <td>Fastai provides easy to use wrappers to quickl...</td>\n",
       "      <td>[code, tutorial, fastai, deep-learning, parkin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id           created_on                                              title  \\\n",
       "0   1  2020-02-17 06:30:41                            Machine Learning Basics   \n",
       "1   2  2020-02-17 06:41:45  Deep Learning with Electronic Health Record (E...   \n",
       "2   3  2020-02-20 06:07:59  Automatic Parking Management using computer vi...   \n",
       "3   4  2020-02-20 06:21:57  Easy street parking using region proposal netw...   \n",
       "4   5  2020-02-20 06:29:18  Deep Learning based parking management system ...   \n",
       "\n",
       "                                         description  \\\n",
       "0  A practical set of notebooks on machine learni...   \n",
       "1  A comprehensive look at recent machine learnin...   \n",
       "2  Detecting empty and parked spaces in car parki...   \n",
       "3  Get a text on your phone whenever a nearby par...   \n",
       "4  Fastai provides easy to use wrappers to quickl...   \n",
       "\n",
       "                                                tags  \n",
       "0  [code, tutorial, keras, pytorch, tensorflow, d...  \n",
       "1    [article, tutorial, deep-learning, health, ehr]  \n",
       "2  [code, tutorial, video, python, machine-learni...  \n",
       "3  [code, tutorial, python, pytorch, machine-lear...  \n",
       "4  [code, tutorial, fastai, deep-learning, parkin...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe\n",
    "df = pd.DataFrame(projects)\n",
    "print (f\"{len(df)} projects\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac19c26e",
   "metadata": {},
   "source": [
    "The reason we want to iteratively add more features is because it introduces more complexity and effort. We may have additional data about each feature such as author info, html from links in the description, etc. While these may have meaningful signal, we want to slowly introduce these after we close the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a4413a",
   "metadata": {},
   "source": [
    "### Auxiliary Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34302538",
   "metadata": {},
   "source": [
    "We're also going to be using an auxiliary dataset which contains a collection of all the tags with their aliases and parent/child relationships. This auxiliary dataset was used by our application to automatically add the relevant parent tags when the child tags were present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50d70b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400 tags\n"
     ]
    }
   ],
   "source": [
    "# Load tags\n",
    "url = \"https://raw.githubusercontent.com/GokuMohandas/MadeWithML/main/datasets/tags.json\"\n",
    "tags = json.loads(urlopen(url).read())\n",
    "tags_dict = {}\n",
    "for item in tags:\n",
    "    key = item.pop(\"tag\")\n",
    "    tags_dict[key] = item\n",
    "print (f\"{len(tags_dict)} tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7e8f019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b087c5c2d8f249f8befceb1036141aca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='tag', index=283, options=('3d', 'action-localization', 'action-recâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@widgets.interact(tag=list(tags_dict.keys()))\n",
    "def display_tag_details(tag='question-answering'):\n",
    "    print (json.dumps(tags_dict[tag], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f7ce16",
   "metadata": {},
   "source": [
    "## Data Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dfd2dc",
   "metadata": {},
   "source": [
    "There are several techniques to mitigate data imbalance, including resampling (oversampling from minority classes / undersampling from majority classes), account for the data distributions via the loss function (since that drives the learning process), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cf7e6a",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c253cf28",
   "metadata": {},
   "source": [
    "We could have used the user provided tags as our labels but what if the user added a wrong tag or forgot to add a relevant one. To remove this dependency on the user to provide the gold standard labels, we can leverage labeling tools and platforms. These tools allow for quick and organized labeling of the dataset to ensure its quality. And instead of starting from scratch and asking our labeler to provide all the relevant tags for a given project, we can provide the author's original tags and ask the labeler to add / remove as necessary. The specific labeling tool may be something that needs to be custom built or leverages something from the ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a0a01c",
   "metadata": {},
   "source": [
    "### General\n",
    "\n",
    "- <a href=\"https://scale.com/\">Scale AI</a>: the data platform for high quality training and validation data for AI applications.\n",
    "- <a href=\"https://github.com/heartexlabs/label-studio\">Label Studio</a>: a multi-type data labeling and annotation tool with standardized output format.\n",
    "- <a href=\"https://github.com/UniversalDataTool/universal-data-tool\">Universal Data Tool</a>: collaborate and label any type of data, images, text, or documents in an easy web interface or desktop app.\n",
    "- <a href=\"https://github.com/explosion/prodigy-recipes\">Prodigy</a>: recipes for the Prodigy, our fully scriptable annotation tool.\n",
    "- <a href=\"https://github.com/janfreyberg/superintendent\">Superintendent</a>: an ipywidget-based interactive labeling tool for your data to enable active learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c342da1e",
   "metadata": {},
   "source": [
    "### Natural language processing\n",
    "\n",
    "- <a href=\"https://github.com/doccano/doccano\">Doccano</a>: an open source text annotation tool for text classification, sequence labeling and sequence to sequence tasks.\n",
    "- <a href=\"https://github.com/nlplab/brat\">BRAT</a>: a rapid annotation tool for all your textual annotation needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f79c87",
   "metadata": {},
   "source": [
    "### Computer Vision\n",
    "- <a href=\"https://github.com/tzutalin/labelImg\">LabelImg</a>: a graphical image annotation tool and label object bounding boxes in images.\n",
    "- <a href=\"https://github.com/openvinotoolkit/cvat\">CVAT</a>: a free, online, interactive video and image annotation tool for computer vision.\n",
    "- <a href=\"https://github.com/Microsoft/VoTT\">VoTT</a>: an electron app for building end-to-end object detection models from images and videos.\n",
    "- <a href=\"https://github.com/SkalskiP/make-sense\">makesense.ai</a>: a free to use online tool for labelling photos.\n",
    "- <a href=\"https://github.com/rediscovery-io/remo-python\">remo</a>: an app for annotations and images management in computer vision.\n",
    "- <a href=\"https://github.com/aralroca/labelai\">Labelai</a>: an online tool designed to label images, useful for training AI models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058e7f40",
   "metadata": {},
   "source": [
    "### Audio\n",
    "- <a href=\"https://github.com/midas-research/audino\">Audino</a>: an open source audio annotation tool for voice activity detection (VAD), diarization, speaker identification, automated speech recognition, emotion recognition tasks, etc.\n",
    "- <a href=\"https://github.com/CrowdCurio/audio-annotator\">audio-annotator</a>: a JavaScript interface for annotating and labeling audio files.\n",
    "- <a href=\"https://github.com/ritazh/EchoML\">EchoML</a>: a web app to play, visualize, and annotate your audio files for machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9dca12",
   "metadata": {},
   "source": [
    "### Miscellaneous\n",
    "- <a href=\"https://github.com/CogStack/MedCAT\">MedCAT</a>: a medical concept annotation tool that can extract information from Electronic Health Records(EHRs) and link it to biomedical ontologies like SNOMED-CT and UMLs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e839b6c",
   "metadata": {},
   "source": [
    "## Active Learning\n",
    "Even with a powerful labeling tool and established workflows, it's easy to see how involved and expensive labeling can be. Therefore, many teams employ active learning to iteratively label the dataset and evaluate the model.\n",
    "1. Label a small, initial dataset to train a model.\n",
    "2. Ask the trained model to predict on some unlabeled data.\n",
    "3. Determine which new data points to label from the unlabeled data based on:\n",
    "   - entropy over the predicted class probabilities.\n",
    "   - samples with lowest predicted, calibrated, confidence (uncertainty sampling)\n",
    "   - discrepancy in predictions from an ensemble of trained models\n",
    "4. Repeat until the desired perdormance is achieved.\n",
    "\n",
    "> This can be significantly more cost-effective and faster than labeling the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c985b",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "- <a href=\"https://github.com/modAL-python/modAL\">ModAL</a>: a modular active learning framework for Python.\n",
    "- <a href=\"https://github.com/ntucllab/libact\">libact</a>: pool-based active learning in Python.\n",
    "- <a href=\"https://github.com/NUAA-AL/ALiPy\">ALiPy</a>: active learning python toolbox, which allows users to conveniently evaluate, compare and analyze the performance of active learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a893dda",
   "metadata": {},
   "source": [
    "## Weak supervision\n",
    "If we had samples that needed labeling or if we simply wanted to validate existing labels, we can use weak supervision to generate labels as opposed to hand labeling all of them. We could utilize weak supervision via labeling functions to label our existing and new data. We can create constructs based on keywords, pattern expressions, knowledge bases and generalized models to create these labeling functions to label our data. And we can add to the labeling functions over time and even mitigate conflicts amongst the different labeling functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e282ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snorkel\n",
      "  Downloading snorkel-0.9.7-py3-none-any.whl (145 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 145 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas<2.0.0,>=1.0.0 in /home/anantvaid/anaconda3/lib/python3.8/site-packages (from snorkel) (1.2.5)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.2.0 in /home/anantvaid/anaconda3/lib/python3.8/site-packages (from snorkel) (1.6.2)\n",
      "Collecting munkres>=1.0.6\n",
      "  Downloading munkres-1.1.4-py2.py3-none-any.whl (7.0 kB)\n",
      "Collecting tensorboard<2.0.0,>=1.14.0\n",
      "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.8 MB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn<0.25.0,>=0.20.2 in /home/anantvaid/anaconda3/lib/python3.8/site-packages (from snorkel) (0.24.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.33.0 in /home/anantvaid/anaconda3/lib/python3.8/site-packages (from snorkel) (4.61.2)\n",
      "Requirement already satisfied: torch<2.0.0,>=1.2.0 in /home/anantvaid/anaconda3/lib/python3.8/site-packages (from snorkel) (1.8.1)\n",
      "Collecting networkx<2.4,>=2.2\n",
      "  Downloading networkx-2.3.zip (1.7 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.7 MB 9.1 MB/s eta 0:00:01     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 696 kB 9.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy<1.20.0,>=1.16.5\n",
      "  Downloading numpy-1.19.5-cp38-cp38-manylinux2010_x86_64.whl (14.9 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.9 MB 67 kB/s  eta 0:00:01     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 11.5 MB 8.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in /home/anantvaid/anaconda3/lib/python3.8/site-packages (from networkx<2.4,>=2.2->snorkel) (5.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/anantvaid/anaconda3/lib/python3.8/site-packages (from pandas<2.0.0,>=1.0.0->snorkel) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/anantvaid/anaconda3/lib/python3.8/site-packages (from pandas<2.0.0,>=1.0.0->snorkel) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/anantvaid/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas<2.0.0,>=1.0.0->snorkel) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/anantvaid/anaconda3/lib/python3.8/site-packages (from scikit-learn<0.25.0,>=0.20.2->snorkel) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/anantvaid/anaconda3/lib/python3.8/site-packages (from scikit-learn<0.25.0,>=0.20.2->snorkel) (1.0.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/anantvaid/anaconda3/lib/python3.8/site-packages (from tensorboard<2.0.0,>=1.14.0->snorkel) (52.0.0.post20210125)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/anantvaid/anaconda3/lib/python3.8/site-packages (from tensorboard<2.0.0,>=1.14.0->snorkel) (3.11.2)\n",
      "Collecting grpcio>=1.6.3\n",
      "  Downloading grpcio-1.40.0-cp38-cp38-manylinux2014_x86_64.whl (4.3 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.3 MB 6.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.4\n",
      "  Downloading absl_py-0.14.0-py3-none-any.whl (131 kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 131 kB 7.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /home/anantvaid/anaconda3/lib/python3.8/site-packages (from tensorboard<2.0.0,>=1.14.0->snorkel) (0.36.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/anantvaid/anaconda3/lib/python3.8/site-packages (from tensorboard<2.0.0,>=1.14.0->snorkel) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions in /home/anantvaid/anaconda3/lib/python3.8/site-packages (from torch<2.0.0,>=1.2.0->snorkel) (3.10.0.0)\n",
      "Building wheels for collected packages: networkx\n",
      "  Building wheel for networkx (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for networkx: filename=networkx-2.3-py2.py3-none-any.whl size=1555990 sha256=754108c5c67d4471af6873f6584676d2e9dc246541e2203318c22ce02e5b262b\n",
      "  Stored in directory: /home/anantvaid/.cache/pip/wheels/ff/62/9e/0ed2d25fd4f5761e2d19568cda0c32716556dfa682e65ecf64\n",
      "Successfully built networkx\n",
      "Installing collected packages: numpy, markdown, grpcio, absl-py, tensorboard, networkx, munkres, snorkel\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.20.2\n",
      "    Uninstalling numpy-1.20.2:\n",
      "      Successfully uninstalled numpy-1.20.2\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 2.5\n",
      "    Uninstalling networkx-2.5:\n",
      "      Successfully uninstalled networkx-2.5\n",
      "Successfully installed absl-py-0.14.0 grpcio-1.40.0 markdown-3.3.4 munkres-1.1.4 networkx-2.3 numpy-1.19.5 snorkel-0.9.7 tensorboard-1.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install snorkel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "117c437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.labeling import labeling_function\n",
    "\n",
    "@labeling_function()\n",
    "def contains_tensorflow(text):\n",
    "    condition = any(tag in text.lower() for tag in (\"tensorflow\", \"tf\"))\n",
    "    return \"tensorflow\" if condition else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fb43b5",
   "metadata": {},
   "source": [
    "## Iteration\n",
    "Labeling isn't just a one time event or something we repeat identically. As new data is available, we'll want to strategically label the appropriate samples and improve slices of our data that are lacking in quality. In fact, there's an entire workflow related to labeling that is initiated when we want to iterate. We'll learn more about this iterative labeling process in our continual learning lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2aa81e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
